{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#combatting-bias","title":"Combatting Bias","text":"<p>Research within the social sciences and humanities (SSH) depends significantly on data sources, many of which are drawn from - and therefore perpetuate - historical biases and inequalities. Combatting Bias is an initiative that aims to critically examine the use of this data, as well as define sustainable, ethical and transparent standards to which future SSH data creation can adhere to. We call these standards FAIR+, a combination of the FAIR principles that make data technically sound; together with +, to ensure data is ethically sound as well. We conduct this research in collaboration with four projects creating datasets from colonial archives and ten advisors from the social sciences and humanities. </p> <p>Combatting Bias is funded by the Dutch Research Council (NWO) via the Thematic Digital Competence Centre Social Sciences &amp; Humanities (TDCC-SSH). For more information about the TDCC-SSH, please visit their website.  </p> Dayanita Singh, Museum of Chance, 2013. \u00a9 Dayanita Singh Dayanita Singh, Museum of Chance, 2013. \u00a9 Dayanita Singh"},{"location":"About/FAIR%2B%20Principles/","title":"FAIR+ Principles","text":"<p>Combatting Bias is aimed to tackle both the technological and ethical complexities surrounding data collection and dataset creation. </p>"},{"location":"About/FAIR%2B%20Principles/#fair","title":"FAIR","text":"<p>For the former, FAIR principles (Findable, Accessible, Interoperable, Reusable) have been established to guide the creation of human (re)usable and machine-usable scholarly data. In the field of SSH, the FAIR principles are a useful guide to make historical data easily (re)usable in the long-term, to the benefit of current and future research. Moreover, FAIR principles emphasise the importance of machine-usability as well. </p> <p>Findable: </p> <ul> <li>Datasets are easily discoverable through clear metadata and unique identifiers.</li> <li>Metadata include diverse terminologies and perspectives.</li> </ul> <p>Accessible:</p> <ul> <li>Data (and metadata) is retrievable through standardised protocols.</li> <li>Access conditions are clearly stated, considering sensitive information and ethical implications.  </li> </ul> <p>Interoperable:</p> <ul> <li>Data uses standardised vocabularies and formats that are critically examined for inclusivity and historical biases.</li> <li>Metadata includes clear descriptions of concepts, potential biases, and ethical considerations to facilitate responsible cross-domain use.</li> </ul> <p>Reusable:</p> <ul> <li>Clear usage licences are provided, along with guidance on ethical reuse and potential pitfalls to avoid.</li> </ul> <p>See more about FAIR: https://www.go-fair.org/ </p>"},{"location":"About/FAIR%2B%20Principles/#_1","title":"+","text":"<p>We believe it is necessary for historical data to be scrutinised at an ethical level. Historical sources are never objective and must be contextualised - the same goes for historical datasets based on those sources. This includes (re)considering certain concepts (such as gender and ethnicity), as well as critically examining the positionalities of the author(s) and researcher(s). This is what is looked at within the \u2018+\u2019.</p> <p>+ (ethical considerations):</p> <ul> <li>Inclusivity: Represent diverse perspectives and experiences</li> <li>Transparency: Disclose data sources, methods, and potential biases</li> <li>Context: Provide necessary historical and cultural information</li> <li>Harm Prevention: Assess and mitigate negative impacts on represented groups</li> <li>Community Involvement: Engage relevant communities in data governance</li> <li>Accountability: Establish feedback and correction mechanisms</li> </ul>"},{"location":"About/Mission/","title":"The Project","text":"<p>Working definition of bias:  Bias (n.) disproportionate weight in favour of or against an idea or thing Biased datasets collections of data that disproportionately represent certain perspectives or groups</p>"},{"location":"About/Mission/#mission","title":"Mission","text":"<p>Our mission is to promote ethical and responsible data practices in the humanities and social sciences by developing comprehensive guidelines for creating datasets that are not only technically sound, but also fair, transparent and inclusive, and respectful of diverse perspectives and experiences.</p> <p>We believe this research is relevant and necessary, as epistemic systems and data creation are deeply intertwined. The increasing use of data in the field of SSH has allowed for more diversity in approaches to research questions, as well as broaden the possible range of questions that can be asked. However, historical data used in SSH research is by nature overwhelmed by biases, impacted by the author\u2019s positionality and context, producing problematic language and categorisations. The complexity of this issue in the field of SSH lies within the fact that these skewed perspectives cannot simply be replaced with less biased alternatives (a \u2018rewriting\u2019 of the archives). Instead, we believe it is the responsibility of the dataset creators to critically reflect on the biases of the source data, and to acknowledge, contextualise and inform users about these. Combatting Bias underlines the necessity for responsible and transparent strategies to navigate these biases. Thus, we aim to create lasting and flexible handholds for dataset creators to guide them in this process. </p> <p>We will therefore produce a set of guidelines for data creation that we call FAIR+. These guidelines will be rooted in the existing FAIR principles, aimed at developing a technically sustainable data creation process; combined with crucial ethical considerations enmeshed in the field of SSH, which have become more pronounced as the field increasingly uses digital infrastructures to support and enable research. This encourages the creation of documentation and datasheets that critically and transparently reflect on our data and data curation practices. In line with the FAIR+ guidelines, we will additionally create reusable templates that can be used for ethical data documentation. Through this, we not only hope to establish an ethical and sustainable framework for future data collection and curation, but also encourage that different, less represented data is collected. </p>"},{"location":"About/Mission/#network-of-expertise","title":"Network of expertise","text":"<p>Combatting Bias is a collaborative project. We are grateful to work closely together with four partner projects and ten advisors, who will shape our guidelines, through insights into their experiences and expertise in dataset creation, (digital) heritage, decolonisation, fair data, and DEI. Through the wide range of perspectives, we will be able to develop comprehensive FAIR+ guidelines that effectively address data bias in SHH. The project itself is also embedded in two research institutes: Huygens Institute and the International Institute of Social History (IISH). </p>"},{"location":"About/Mission/#objectives","title":"Objectives","text":"<p>In short, the objectives of the project are threefold: </p> <ol> <li>Identify biases: critically reflect on existing datasets and the dataset creation process. This leads to insights into what is disproportionately represented in datasets.</li> <li>Mitigate biases: develop guidelines to aid researchers in creating ethical and sustainable datasets through transparent, contextual and FAIR documentation. </li> <li>Promote: actively encourage the use of dataset creation guidelines in SSH research and active (re)consideration of certain concepts (such as gender and ethnicity) used in SSH datasets. Additionally, data collection of underrepresented perspectives is encouraged to be undertaken to mitigate biases. </li> <li>Build a network: exchange knowledge and experiences with experts from different fields within SSH </li> </ol>"},{"location":"About/Objectives/","title":"Objectives","text":"<p>The objectives of the project are threefold: </p> <ol> <li>Identify and analyse biases in existing datasets within the field of SSH;</li> <li>Develop a set of guidelines for creating FAIR+ datasets in SSH;</li> <li>Promote the adoption of these guidelines in the field.</li> </ol>"},{"location":"About/The%20Project/","title":"The Project","text":"<p>Research within the social sciences and humanities (SSH) depends significantly on data sources, many of which are drawn from - and therefore perpetuate - historical biases and inequalities. Combatting Bias is an initiative that aims to critically examine the use of this data, as well as define sustainable, ethical and transparent standards to which future SSH data creation can adhere to. We call these standards FAIR+, a combination of the FAIR principles that make data technically sound; together with +, to ensure data is ethically sound as well. We conduct this research in collaboration with four projects creating datasets from colonial archives and ten advisors from the social sciences and humanities. </p> <p>Combatting Bias is funded by the Dutch Research Council (NWO) via the Thematic Digital Competence Centre Social Sciences &amp; Humanities (TDCC-SSH).</p>"},{"location":"Contact/Contact/","title":"Contact","text":"<p>We value your input and are always eager to hear from you. Whether you have feedback about our project, questions you\u2019d like answered, or are interested in potential collaboration opportunities, please reach out!</p> <p>You can contact us via email at: combattingbias[at]gmail[dot]com</p>"},{"location":"FAQ/FAQ/","title":"FAQ","text":"<p>Who should be interested in the work of the project?  The project is primarily aimed at dataset creators working in the field of SSH, practically providing guidance in the process of creating ethical historical data. The ethical data will be transparent about skewed categorisations and language the historical sources used, resulting in the prevention of the perpetuation of these (harmful) biases in the future reiterations of the data. Furthermore, it will allow current researchers to reconsider concepts in their data, reading it \u2018against the grain\u2019, and gain insight into different narratives and perspectives their historical data contains as well as which are lacking - this stimulates new research ventures. </p> <p>We, however, hope to make a broader impact than only within SSH research circles. In line with the efforts within the humanities towards decolonisation of knowledge, academia, and cultural heritage (institutions), Combatting Bias focusses these efforts on digital heritage. The project is therefore engaging in issues that may interest heritage specialists and institutions, inclusion and diversity activists, archivists among others. The general public too, as direct users of historical datasets through archives (Nationaal Archief) for example, or indirect users, such as reading articles or watching documentaries basing themselves on historical datasets, can find interest in the project. </p> <p>Can projects that are not partnered with Combatting Bias get in touch for guidance and advise about creating fair and ethical data?  Yes! We are open to feedback - especially if you have experience in the fields of fair data, digital humanities, DEI, (digital) heritage, (colonial) history. </p> <p>If you are a user of historical datasets, we would also like to hear from you! Datasets are ultimately used by people - hence, while focusing on the ethical integrity of historical datasets, it is also important to us that the datasets and their attached documentation are easy to use. We are interested in what is important and/or challenging to different groups of users when engaging with data. </p> <p>Find out how to contact us here.</p> <p>How long will the project run for?  The duration of the Combatting Bias project is 12 months (September 2024 - September 2025). </p>"},{"location":"News/Kickoff/","title":"Combatting Bias kicks off","text":"<p>On 8 October 2024, we kicked off the Combatting Bias project at the Huygens Institute in Amsterdam. The event brought together our wide network of partners and advisors, both in-person and online. </p> <p>The day began with project introductions and research presentations, highlighting our shared ambition to create FAIR and ethical datasets for the humanities. The diversity of approaches and expertise within our network was immediately apparent, setting the stage for rich discussions. If you\u2019re curious about who\u2019s involved, you can find short descriptions of our partner projects and advisors on our website. </p> Group picture of (most of the) kickoff participants <p>After the introductory presentations, participants engaged in breakout sessions where we delved into two datasets based on colonial sources: </p> <ol> <li>Nationaal Archief\u2019s Chinese contract labourers in Suriname</li> <li>GLOBALISE\u2019s Polities dataset</li> </ol> <p>The discussions focussed on three main questions:  </p> <ol> <li>What is bias?</li> <li>Can you identify biases within this dataset?</li> <li>How is this bias combatted in the dataset? Can it be combatted?</li> </ol> <p>The breakout sessions were eye-opening and raised some crucial points on creating ethical and sustainable datasets: </p>"},{"location":"News/Kickoff/#1-balancing-openness-and-sensitivity","title":"1. Balancing Openness and Sensitivity","text":"<p>First, we all agreed that datasets should adhere to the FAIR principles. This makes the data easy to search and use for both humans and computers. However, we also realised that this isn\u2019t always straightforward, especially when dealing with sensitive personal information from colonial sources. This thus left us with the question: should data be open and accessible, even when it may perpetuate harmful stereotypes and/or disclose personal information?</p>"},{"location":"News/Kickoff/#2-effective-documentation","title":"2. Effective Documentation","text":"<p>Another key topic was the importance of good documentation. We agreed that thorough records are crucial for transparency and usability. Ideally, documentation should cover various aspects, including but not limited to: the aim of the dataset, definitions of variables and historical terms, content and context of sources, decision-making processes that influenced dataset creation, and its potential biases.</p> <p>However, we face the challenge of balancing thoroughness with accessibility. While we aim for transparency, overly extensive documentation can be overwhelming. Most users won\u2019t read through pages of information. Our challenge lies in deciding what information is most important to highlight and how to present it effectively for various users.</p>"},{"location":"News/Kickoff/#3-diverse-perspectives-are-essential","title":"3. Diverse Perspectives are Essential","text":"<p>Lastly, we talked about the need to include diverse perspectives in our work. Participants highlighted that bias often stems from a lack of diversity in these processes. Even well-documented datasets can be problematic if they rely solely on Dutch or Anglophone sources, potentially missing crucial perspectives from the people central to the historical narratives. Colonial archives present additional challenges, such as westernised spellings of names and places. This not only misrepresents the original information, but also makes the data harder to find without alternative spellings. It is therefore crucial to include not only a wider range of secondary literature from different regions, but also to consult people who are language and culture experts on the region the source is describing. </p> <p>The group activity itself, with its many participants examining the data from various angles, was a case in point. It highlighted how diverse perspectives can reveal different kinds of biases.</p> Teams deep in dataset analysis <p>The afternoon was a very encouraging start to the project. We are grateful to all the participants for their thought-provoking input and attendance. Special thanks go to Sebastiaan Derks, Head of the Data Management Department of the Huygens Institute, and Nils Arlinghaus, Community Coordinator at the TDCC-SSH. Their presentations helped explain how Combatting Bias fits into wider research agendas. We also appreciate our colleagues from the GLOBALISE team who helped out during the breakout session.</p> <p>As we conclude this successful kickoff, we\u2019re energised by the challenges ahead. Creating ethical, accessible, and usable datasets in the humanities and social sciences is no small task, but it\u2019s one we\u2019re eager to tackle!</p>"},{"location":"News/Kickoff_Announcement/","title":"Kickoff Announcement","text":"<p>Project Kick-Off | 8 October 2024  The Combatting Bias project is organising an official kick-off to launch the project to its partner projects and advisors. It will be an interactive 3-hour event where the project and all collaborators have the opportunity to meet, discuss (common) challenges faced in dataset creation in their work, and think about what collaboration will look like. </p> <p>The event will be held on 8 October, 14:00-17:00.</p>"},{"location":"Team/Team/","title":"Team","text":"<p>Lodewijk Petram  serves as the Project Leader for the Combatting Bias project, overseeing the project\u2019s finances and managing all communications with NWO, the funding organisation. As a co-applicant of the GLOBALISE project and currently its Project Manager, he is deeply invested in making colonial archives more accessible and supporting new research that utilises these historical resources.</p> <p>Manjusha Kuruppath  is the co-investigator and coordinator in the Combatting Bias project. She is a historian of the Dutch East India Company and has worked as the Post-Doctaral Team Lead in Historical Contextualization on the GLOBALISE project since 2022. This role has acquainted her with the challenges that plague data in the humanities and urgency to find solutions for them. This constitutes the basis for her work on the Combatting Bias project. </p> <p>Richard Zijdeman  is the Project Leadership Advisor for Combattign Bias. He is the head of the Data &amp; Augmentation department at the International Institute of Social History (IISH) and directs the HSNDB. Richard\u2019s work sits at the interection between history and sociology, cultural heritage and Knowledge Representation (AI), focusing on creating digital infrastructures that help uncover and connect data sources for the study of long term inequality.</p> <p>Mrinalini Luthra  is a Data Ethics Specialist on the Combatting Bias project. She engages with questions around responsible stewardship of historical data about colonisation and slavery in digital spaces and how to express the subjectivities and relationality of data, technologies, and interfaces. Trained as a mathematician, philosopher, and logician, her interests lie at the intersection of technology, ethics, and design.</p> <p>Amber Zijlma  works as a Data Steward (Data Governance) on the Combatting Bias project, evaluating data governance practices and strategies of dataset creation on their ethical sustainability. Her previous research has focused on global and colonial histories, with a special interest in the positionality of Chinese labour migrants in European colonial societies within Asia and Africa.</p> <p>You can email us at [firstname].[lastname]@huygens.knaw.nl</p> <p>The collaborative structure of the Combatting Bias project is visualised below through a three-tiered circular diagram. The innermost circle features the core members, directly involved in the project\u2019s daily operations. The second circle displays the four partner projects, indicating close collaborations. The outermost circle categorises the advisor projects by their expertise areas, showcasing the extensive support network and the diverse knowledge contributing to the project.</p> <p></p> <p>We welcome further collaborations and guidance, so please get in touch with us!</p>"},{"location":"Team/Partners/Advisors/","title":"Advisors","text":""},{"location":"Team/Partners/Advisors/#prof-charles-jeurgens","title":"Prof. Charles Jeurgens","text":"<p>Charles is professor of archival sciences at the University of Amsterdam. His work bridges historical insights and contemporary challenges in information management and transparency. His expertise will help ensure that the guidelines developed by Combatting Bias are grounded in its historical and social context and address the power dynamics inherent in (archival) data. </p>"},{"location":"Team/Partners/Advisors/#jeftha-pattikawa","title":"Jeftha Pattikawa","text":"<p>Jeftha Pattikawa is a project manager Diversity, Equity, and Inclusion (DEI) and curator of the public program at the National Archives of the Netherlands. He is also an advisory board member of Europeana, which empowers the cultural heritage sector in its digital transformation. His expertise will help ensure the project\u2019s outcomes address contemporary challenges of DEI, with regards representation in digital datasets. </p>"},{"location":"Team/Partners/Advisors/#dr-caroline-drieenhuizen","title":"Dr. Caroline Drie\u00ebnhuizen","text":"<p>Dr. Caroline Drie\u00ebnhuizen is assistant professor of Cultural History at the Faculty of Law and Humanities, Open University of the Netherlands. Her focus is on the European cultural aspects of colonialism and Indonesia\u2019s decolonisation, approached through a museological and object-driven perspective in Indonesia, the Netherlands, and Europe. Dr. Drie\u00ebnhuizen will help offer critical insights into ethical considerations and narrative framing within the project.</p>"},{"location":"Team/Partners/Advisors/#de-bias-netherlands-institute-of-sound-and-vision","title":"DE-BIAS, Netherlands Institute of Sound and Vision","text":"<p>The DE-BIAS project detects offensive language and suggests alternative language within these collections, leading to more informed wording. Their work on detecting and addressing offensive language will guide the project in developing sensitive and inclusive metadata practices.</p> <p>Stevie Nolten is a researcher working on coloniality and institutional accountability. Apart from research, she is also a councillor for BIJ1 and group chairperson in Utrecht.</p> <p>Isabel Beirigo is a research communications specialist working on the DE-BIAS project and is a member of the working group on decolonisation, at the International Council of Museums (ICOM).</p> <p>Monique Groot is a product manager of research and cultural heritage. </p>"},{"location":"Team/Partners/Advisors/#layan-nijem","title":"Layan Nijem","text":"<p>Layan is the project coordinator for the project Decolonizing South East Asian Sounds (DeCoSEAS). Their focus on non-textual sources of knowledge challenges traditional data paradigms, and will enrich the project with broader methodological perspectives. </p>"},{"location":"Team/Partners/Advisors/#district-six-museum","title":"District Six Museum","text":"<p>The museum, located in the former residential area of District Six in Cape Town, South Africa, was established in 1994 as a tribute to the 60,000 forcibly relocated residents of various races during the 1970s Apartheid era. It not only commemorates this period but also actively participates in the reconstruction of the neighbourhood. These efforts aim to foster a community where dignity, identity, and the coexistence of different races are respected and celebrated. Their experiences in memorialisation and community engagement through the lens of apartheid\u2019s history will inform the project\u2019s approach to addressing historical injustices and biases.</p> <p>The two advisors from District Six Museum are:</p> <p>Mandy Sanger, as the Head of Education at the District Six Museum, facilitates youth and community engagement through creative, hands-on experiences. Her work focuses on fostering resilience and solidarity, challenging societal norms of power and privilege, and promoting an egalitarian future through anti-racism programs and reimagining public spaces with a humanising approach.</p> <p>Tina Smith, Head of Collections at the District Six Museum, has played a pivotal role in the development of the museum\u2019s curatorial framework and her work seeks to address ways of rethinking notions of memorialisation.</p>"},{"location":"Team/Partners/Advisors/#fair-expertise-hub-open-data-infrastructure-for-social-science-and-economic-innovations-odissei","title":"FAIR Expertise Hub, Open Data Infrastructure for Social Science and Economic Innovations (ODISSEI)","text":"<p>The FAIR Expertise Hub help communities establish their data management plans for adapting FAIR principles using their FAIR Implementation Profile. Their expertise will ensure that the project\u2019s guidelines are technically sound and its implementation is aligned with FAIR principles.</p> <p>The two advisors from this project are:</p> <p>Angelica M. Maineri is a data manager at ODISSEI, where she works to enhance data accessibility for the social science research community, implementing FAIR principles and developing resources like the ODISSEI code library.</p> <p>Shuai Wang is a scientific engineer at the FAIR Expertise Hub.</p>"},{"location":"Team/Partners/Advisors/#global-data-lab-gdl","title":"Global Data Lab (GDL)","text":"<p>GDL, situated at the Nijmegen School of Management, Radboud University, is an independent centre focusing on data and research. Utilising over 500 household survey datasets, GDL has created a comprehensive database encompassing information on 35 million individuals across 130+ countries, primarily in low- and middle-income regions. GDL\u2019s work involves conducting research and developing specialised databases, indices, and tools for monitoring and analysing societal status and progress. GDL\u2019s work on societal status and progress in low- and middle-income countries will inform the project\u2019s efforts to create inclusive data guidelines for the social sciences that reflect diverse global contexts and contribute to addressing historical and contemporary inequalities.</p> <p>Prof. Natascha Wagner is a professor of International Economics at the Nijmegen School of Management at Radboud University, Netherlands and the director of the GDL. </p>"},{"location":"Team/Partners/Advisors/#haicu","title":"HAICU","text":"<p>The HAICu project brings together AI and Digital Humanities researchers, heritage professionals, and interested citizens to work together to achieve scientific breakthroughs in order to open up, link, and analyse large-scale digital heritage collections. </p> <p>Prof. Julia Noordegraaf is professor of Digital Heritage and director of the Amsterdam Center of Cultural Heritage and Identity (ACHI) at the University of Amsterdam. </p>"},{"location":"Team/Partners/Advisors/#hub-global-labour-conflicts","title":"Hub Global Labour Conflicts","text":"<p>The Hub Global Labour Conflicts provides scholars, students, activists, and other interested audiences with historical and contemporary data on workers\u2019 collective actions. The project is currently expanding its scope to include data from all over the world, as well as reconsidering concepts around labour. It aims to be a central resource for freely accessible global labour conflict data.</p> <p>Jens Aurich is a junior researcher at the International Institute of Social History. For the Hub Global Labour Conflicts, he develops digital workflows for the collection of data on collective action from digitized sources.</p>"},{"location":"Team/Partners/Partner%20Projects/","title":"Projects","text":"<p>Combatting Bias works closely with partner projects, which will play a pivotal role in data sharing, collaboration, and knowledge exchange. These partner projects have a shared focus on data related to colonialism and slavery and as a practical matter, already deal with:</p> <ol> <li> <p>Dataset Creation: They specialise in creating and organising data sourced from colonial archives, encompassing named and unnamed individuals, places, and polities across Asia, Africa, and the Americas.</p> </li> <li> <p>Addressing Historical Imbalances: The partner projects work to counterbalance archival silences and rectify historiographic imbalances.</p> </li> <li> <p>Ethical Considerations: The coalition is deeply committed to addressing ethical dilemmas inherent in creating and (re)presenting data related to colonial histories.</p> </li> </ol> <p>Our partner projects are:</p>"},{"location":"Team/Partners/Partner%20Projects/#slave-voyages","title":"Slave Voyages","text":"<p>The Slave Voyages project is a comprehensive digital resource that documents the forced transportation of over 12 million enslaved Africans. It combines the Trans-Atlantic and Intra-American slave trade databases, representing decades of collaborative research by international scholars. Developed by a multidisciplinary team, the project features datasets, historical information, and interactive tools. Developed at various universities, the Slave Voyages website is now hosted at Rice University, serving as a crucial resource for understanding the history and impact of the slave trade.</p> <p></p>"},{"location":"Team/Partners/Partner%20Projects/#historische-database-van-suriname-de-cariben","title":"Historische Database van Suriname &amp; de Cariben","text":"<p>The Historische Database van Suriname &amp; de Cariben (Historical Database of Suriname &amp; the Caribbean) project started with the creation of an extensive database that integrates nineteenth and twentieth century source information about formerly enslaved persons in the Dutch colonies in the Americas. It is a collaboration between the Radboud University in Nijmegen, Anton de Kom University in Suriname, the National Archives of the Netherlands and Suriname, heritage organisations and crowdsourcing initiatives. Building on this foundation, researchers from both universities are now focusing on the creation of a database of the Suriname population between 1830-1950, in the project Legacies of Bondage. This will shed light on not only the social processes and diversity of a colonial society, but also the impact of unfree labour on following generations, and the interaction of different groups of migrants in Suriname. Throughout its existence, the Historische Database van Suriname &amp; de Cariben has focused a lot on the public use of its resources, including crowdsourcing in the process of opening up the archives. </p> <p></p>"},{"location":"Team/Partners/Partner%20Projects/#exploring-slave-trade-in-asia-esta","title":"Exploring Slave Trade in Asia (ESTA)","text":"<p>ESTA is building a database that reconstructs the historic slave trade in the Indian Ocean and Maritime Asia region. By highlighting this region of the slave trade, it stimulates new research, as well as connections and more global perspectives to the global trade of enslaved people. The database includes the reconstruction of a variety of forced labour transportations, such as the commodified and legalized slave trade to \u201cillegal\u201d (private) human trafficking, and tributary exchanges between polities to penal slavery. It is through the inclusion of all these forms of bonded labour that global connections can be drawn and the structures of slave trade can be understood. </p> <p></p>"},{"location":"Team/Partners/Partner%20Projects/#globalise","title":"GLOBALISE","text":"<p>The GLOBALISE project is building a digital infrastructure of the Dutch East India Company Archives, to make these archives accessible to the public and researchers. These archives provide insight into early modern societies in Asia, Africa, and Australia. As few archival sources of the time exist of these regions, the archives hold valuable information that illuminates their global interactions with other areas. GLOBALISE\u2019s mission is to empower researchers and the general public to explore these archives and write new, inclusive histories. In order to do so, they are working with AI methods to transcribe over 5 million handwritten pages, extracting key entities and events from the text, and creating reference guides to support their data. </p> <p></p>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/","title":"Word2Vec Experiment","text":"<p>Download links:</p> <ul> <li>Notebook: https://github.com/globalise-huygens/lab.globalise.huygens.knaw.nl/blob/main/docs/experiments/GLOBALISE_Word2Vec_Lab.ipynb</li> <li>Pretrained model (100 dimensions, 645MB): https://surfdrive.surf.nl/files/index.php/s/XmUIlsy33vpRdCX</li> </ul> <p>Downloading this notebook will allow you to experiment with a Word2Vec model based on the GLOBALISE Transcription (V2). You can train the model on your own data, or use the pretrained model to find similar words.</p> <p>If you use the pretrained model, download and unzip the <code>GLOBALISE.word2vec.zip</code> in a <code>data</code> directory in the same folder as this notebook. Only run the first cell, and skip to 'Loading a pretrained model' section to load the model and start experimenting.</p> In\u00a0[6]: Copied! <pre>import os\nimport sys\nimport logging\nimport pickle\n\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.corpora.textcorpus import TextDirectoryCorpus\n\nfrom gensim.corpora.dictionary import Dictionary\n\nfrom gensim.parsing.preprocessing import (\n    remove_stopword_tokens,\n    remove_short_tokens,\n    lower_to_unicode,\n    strip_multiple_whitespaces,\n)\nfrom gensim.utils import deaccent, simple_tokenize, effective_n_jobs\n\nlogging.basicConfig(\n    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n)\nlogging.getLogger().setLevel(logging.INFO)\n\n# Setting\nvector_size = 100\n</pre> import os import sys import logging import pickle  from gensim.models import Word2Vec, KeyedVectors from gensim.corpora.textcorpus import TextDirectoryCorpus  from gensim.corpora.dictionary import Dictionary  from gensim.parsing.preprocessing import (     remove_stopword_tokens,     remove_short_tokens,     lower_to_unicode,     strip_multiple_whitespaces, ) from gensim.utils import deaccent, simple_tokenize, effective_n_jobs  logging.basicConfig(     format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO ) logging.getLogger().setLevel(logging.INFO)  # Setting vector_size = 100 In\u00a0[5]: Copied! <pre>! mkdir -p data &amp;&amp; wget https://datasets.iisg.amsterdam/api/access/datafile/33172?gbrecs=true -O data/globalise_transcriptions_v2_txt.tab --content-disposition\n</pre> ! mkdir -p data &amp;&amp; wget https://datasets.iisg.amsterdam/api/access/datafile/33172?gbrecs=true -O data/globalise_transcriptions_v2_txt.tab --content-disposition <pre>--2024-07-22 22:01:53--  https://datasets.iisg.amsterdam/api/access/datafile/33172?gbrecs=true\nResolving datasets.iisg.amsterdam (datasets.iisg.amsterdam)... 195.169.88.174\nConnecting to datasets.iisg.amsterdam (datasets.iisg.amsterdam)|195.169.88.174|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 399793 (390K) [text/plain]\nSaving to: \u2018data/globalise_transcriptions_v2_txt.tab\u2019\n\ndata/globalise_tran 100%[===================&gt;] 390,42K  --.-KB/s    in 0,06s   \n\n2024-07-22 22:01:53 (6,54 MB/s) - \u2018data/globalise_transcriptions_v2_txt.tab\u2019 saved [399793/399793]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>! mkdir -p data/txt &amp;&amp; wget -i data/globalise_transcriptions_v2_txt.tab -P data/txt/ --content-disposition\n</pre> ! mkdir -p data/txt &amp;&amp; wget -i data/globalise_transcriptions_v2_txt.tab -P data/txt/ --content-disposition <p>We now have a collection of text files, in which each file represents the text per inventory number.</p> <p>The files need a bit of pre-processing before we can work with it. What needs to be done:</p> <ul> <li>Remove all lines starting with <code>#+ </code>. These are comments and not part of the text.</li> </ul> In\u00a0[7]: Copied! <pre>def preprocess_txt(file_path):\n    print(\"Processing\", file_path)\n\n    # Open the textfile\n    with open(file_path) as infile:\n        text = infile.read()\n\n    lines = []\n    for line in text.split(\"\\n\"):\n        if line.startswith(\"#+ \"):\n            continue\n        else:\n            lines.append(line)\n\n    text = \"\\n\".join(lines)\n\n    # Save the cleaned version\n    with open(file_path, \"w\") as outfile:\n        outfile.write(text)\n</pre> def preprocess_txt(file_path):     print(\"Processing\", file_path)      # Open the textfile     with open(file_path) as infile:         text = infile.read()      lines = []     for line in text.split(\"\\n\"):         if line.startswith(\"#+ \"):             continue         else:             lines.append(line)      text = \"\\n\".join(lines)      # Save the cleaned version     with open(file_path, \"w\") as outfile:         outfile.write(text) In\u00a0[\u00a0]: Copied! <pre>FOLDER = \"data/txt\"\n\nfor f in os.listdir(FOLDER):\n    filepath = os.path.join(FOLDER, f)\n    preprocess_txt(filepath)\n</pre> FOLDER = \"data/txt\"  for f in os.listdir(FOLDER):     filepath = os.path.join(FOLDER, f)     preprocess_txt(filepath) <p>Now that we have the data in a usable format, we can start processing it. We will use the Gensim library to train a Word2Vec model on the text data. For this, we first create a Corpus object that will be used to feed text to the model. We use a custom implementation of the <code>gensim.corpora.textcorpus.TextCorpus</code> class to now have a cutoff for the number of words in the vocabulary (standard settings).</p> In\u00a0[9]: Copied! <pre>logger = logging.getLogger(__name__)\n\n\nclass CustomTextDirectoryCorpus(TextDirectoryCorpus):\n    \"\"\"\n    Custom class to set the `prune_at` gensim.Dictionary parameter.\n    \"\"\"\n\n    def __init__(\n        self,\n        input,\n        dictionary=None,\n        metadata=False,\n        character_filters=None,\n        tokenizer=None,\n        token_filters=None,\n        min_depth=0,\n        max_depth=None,\n        pattern=None,\n        exclude_pattern=None,\n        lines_are_documents=False,\n        encoding=\"utf-8\",\n        dictionary_prune_at=2_000_000,\n        **kwargs,\n    ):\n        self._min_depth = min_depth\n        self._max_depth = sys.maxsize if max_depth is None else max_depth\n        self.pattern = pattern\n        self.exclude_pattern = exclude_pattern\n        self.lines_are_documents = lines_are_documents\n        self.encoding = encoding\n\n        self.dictionary_prune_at = dictionary_prune_at\n\n        self.input = input\n        self.metadata = metadata\n\n        self.character_filters = character_filters\n        if self.character_filters is None:\n            self.character_filters = [\n                lower_to_unicode,\n                deaccent,\n                strip_multiple_whitespaces,\n            ]\n\n        self.tokenizer = tokenizer\n        if self.tokenizer is None:\n            self.tokenizer = simple_tokenize\n\n        self.token_filters = token_filters\n        if self.token_filters is None:\n            self.token_filters = [remove_short_tokens, remove_stopword_tokens]\n\n        self.length = None\n        self.dictionary = None\n        self.init_dictionary(dictionary)\n\n        super(CustomTextDirectoryCorpus, self).__init__(\n            input, self.dictionary, metadata, **kwargs\n        )\n\n    def init_dictionary(self, dictionary):\n        \"\"\"Initialize/update dictionary.\n\n        Parameters\n        ----------\n        dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n            If a dictionary is provided, it will not be updated with the given corpus on initialization.\n            If None - new dictionary will be built for the given corpus.\n\n        Notes\n        -----\n        If self.input is None - make nothing.\n\n        \"\"\"\n\n        self.dictionary = dictionary if dictionary is not None else Dictionary()\n\n        if self.input is not None:\n            if dictionary is None:\n                logger.info(\"Initializing dictionary\")\n                metadata_setting = self.metadata\n                self.metadata = False\n                self.dictionary.add_documents(\n                    self.get_texts(), prune_at=self.dictionary_prune_at\n                )\n                self.metadata = metadata_setting\n            else:\n                logger.info(\"Input stream provided but dictionary already initialized\")\n        else:\n            logger.warning(\n                \"No input document stream provided; assuming dictionary will be initialized some other way.\"\n            )\n</pre> logger = logging.getLogger(__name__)   class CustomTextDirectoryCorpus(TextDirectoryCorpus):     \"\"\"     Custom class to set the `prune_at` gensim.Dictionary parameter.     \"\"\"      def __init__(         self,         input,         dictionary=None,         metadata=False,         character_filters=None,         tokenizer=None,         token_filters=None,         min_depth=0,         max_depth=None,         pattern=None,         exclude_pattern=None,         lines_are_documents=False,         encoding=\"utf-8\",         dictionary_prune_at=2_000_000,         **kwargs,     ):         self._min_depth = min_depth         self._max_depth = sys.maxsize if max_depth is None else max_depth         self.pattern = pattern         self.exclude_pattern = exclude_pattern         self.lines_are_documents = lines_are_documents         self.encoding = encoding          self.dictionary_prune_at = dictionary_prune_at          self.input = input         self.metadata = metadata          self.character_filters = character_filters         if self.character_filters is None:             self.character_filters = [                 lower_to_unicode,                 deaccent,                 strip_multiple_whitespaces,             ]          self.tokenizer = tokenizer         if self.tokenizer is None:             self.tokenizer = simple_tokenize          self.token_filters = token_filters         if self.token_filters is None:             self.token_filters = [remove_short_tokens, remove_stopword_tokens]          self.length = None         self.dictionary = None         self.init_dictionary(dictionary)          super(CustomTextDirectoryCorpus, self).__init__(             input, self.dictionary, metadata, **kwargs         )      def init_dictionary(self, dictionary):         \"\"\"Initialize/update dictionary.          Parameters         ----------         dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional             If a dictionary is provided, it will not be updated with the given corpus on initialization.             If None - new dictionary will be built for the given corpus.          Notes         -----         If self.input is None - make nothing.          \"\"\"          self.dictionary = dictionary if dictionary is not None else Dictionary()          if self.input is not None:             if dictionary is None:                 logger.info(\"Initializing dictionary\")                 metadata_setting = self.metadata                 self.metadata = False                 self.dictionary.add_documents(                     self.get_texts(), prune_at=self.dictionary_prune_at                 )                 self.metadata = metadata_setting             else:                 logger.info(\"Input stream provided but dictionary already initialized\")         else:             logger.warning(                 \"No input document stream provided; assuming dictionary will be initialized some other way.\"             ) In\u00a0[10]: Copied! <pre>class SentencesIterator:\n    def __init__(self, generator_function):\n        self.generator_function = generator_function\n        self.generator = self.generator_function()\n\n    def __iter__(self):\n        # reset the generator\n        self.generator = self.generator_function()\n        return self\n\n    def __next__(self):\n        result = next(self.generator)\n        if result is None:\n            raise StopIteration\n        else:\n            return result\n</pre> class SentencesIterator:     def __init__(self, generator_function):         self.generator_function = generator_function         self.generator = self.generator_function()      def __iter__(self):         # reset the generator         self.generator = self.generator_function()         return self      def __next__(self):         result = next(self.generator)         if result is None:             raise StopIteration         else:             return result <p>With the above code we can generate our own 'corpus' object with a slightly bigger dictionary size than in Gensim's standard library. We set it to 20M, since we are also interested in the lesser frequent words (e.g. spelling varieties). We can filter later on minimum frequency.</p> In\u00a0[11]: Copied! <pre>corpus = CustomTextDirectoryCorpus(FOLDER, dictionary_prune_at=20_000_000)\n</pre> corpus = CustomTextDirectoryCorpus(FOLDER, dictionary_prune_at=20_000_000) <pre>2024-07-22 22:14:08,818 : INFO : Initializing dictionary\n2024-07-22 22:14:09,148 : INFO : adding document #0 to Dictionary&lt;0 unique tokens: []&gt;\n2024-07-22 22:36:27,988 : INFO : built Dictionary&lt;10195707 unique tokens: ['__o', '_os', 'aad', 'aag', 'aagwit']...&gt; from 6893 documents (total 694347987 corpus positions)\n2024-07-22 22:36:27,988 : INFO : Input stream provided but dictionary already initialized\n</pre> <p>Now let's save the corpus object to disk, so we can use it later on and don't have to re-run the pre-processing steps. Comment and uncomment the respective code below to run the pre-processing steps or load the corpus object from disk.</p> In\u00a0[12]: Copied! <pre>with open(\"data/corpus.pkl\", \"wb\") as f:\n    pickle.dump(corpus, f)\n</pre> with open(\"data/corpus.pkl\", \"wb\") as f:     pickle.dump(corpus, f) In\u00a0[13]: Copied! <pre># with open(\"data/corpus.pkl\", \"rb\") as f:\n#     corpus = pickle.load(f)\n</pre> # with open(\"data/corpus.pkl\", \"rb\") as f: #     corpus = pickle.load(f) <p>Next step is to train the Word2Vec model. For this, we need to feed it the corpus object multiple times. We do so by initializing an iterator:</p> In\u00a0[14]: Copied! <pre>texts = SentencesIterator(corpus.get_texts)\n</pre> texts = SentencesIterator(corpus.get_texts) <p>Now, let's create a Word2Vec embedding. You can set the number of workers to your CPU count (minus 1). Again, this can take a while.</p> <p>You can experiment with the parameters of the Word2Vec model, such as the vector size, window size, and minimum frequency, but this can lead to a bigger model, longer training time, and not necessarily better results.</p> In\u00a0[\u00a0]: Copied! <pre>workers = effective_n_jobs(max(os.cpu_count() - 1, 1))\nw2v = Word2Vec(\n    texts, vector_size=vector_size, window=5, min_count=5, workers=workers, epochs=5\n)\n</pre> workers = effective_n_jobs(max(os.cpu_count() - 1, 1)) w2v = Word2Vec(     texts, vector_size=vector_size, window=5, min_count=5, workers=workers, epochs=5 ) <p>Now, let's save the embedding for future use. (Similarly, there is a function to load a previously saved model again below)</p> In\u00a0[16]: Copied! <pre>w2v.wv.save_word2vec_format(f\"data/GLOBALISE_{vector_size}.word2vec\")\n</pre> w2v.wv.save_word2vec_format(f\"data/GLOBALISE_{vector_size}.word2vec\") <pre>2024-07-23 00:23:10,897 : INFO : storing 1369250x100 projection weights into data/GLOBALISE.word2vec\n</pre> In\u00a0[5]: Copied! <pre>w2v = KeyedVectors.load_word2vec_format(f\"data/GLOBALISE_{vector_size}.word2vec\")\n</pre> w2v = KeyedVectors.load_word2vec_format(f\"data/GLOBALISE_{vector_size}.word2vec\") <pre>2024-07-26 11:53:24,202 : INFO : loading projection weights from data/GLOBALISE.word2vec\n2024-07-26 11:54:03,938 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (1369250, 100) matrix of type float32 from data/GLOBALISE.word2vec', 'binary': False, 'encoding': 'utf8', 'datetime': '2024-07-26T11:54:03.938194', 'gensim': '4.3.0', 'python': '3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-116-generic-x86_64-with-glibc2.35', 'event': 'load_word2vec_format'}\n</pre> <p>Now, this is where the fun starts. We can use the Word2Vec model to find similar words to a given word and thereby find words that share the same semantics and context.</p> <p>See the Gensim documentation for more information on how to use the Word2Vec model: https://radimrehurek.com/gensim/models/word2vec.html. Below are some examples of how to use the model. You can substitute the words with any word you like, as long as it is in the vocabulary of the model/corpus. Everything needs to be in lowercase.</p> In\u00a0[7]: Copied! <pre>for i in w2v.most_similar(\"pantchialang\", topn=100):\n    print(i[0], end=\" | \")\n</pre> for i in w2v.most_similar(\"pantchialang\", topn=100):     print(i[0], end=\" | \") <pre>pantchialling | pantjall | dehaij | pantch | depantjall | patchiall | pantchiall | challang | debijl | noodhulp | goudsoeker | pantsch | haaij | tapko | pantchialt | jaarvogel | depantchiall | jongedirk | buijtel | krankte | windbuijl | depantjallang | patchiallang | zuykermaalder | pantchallang | depantch | onbeschaamdh | copjagt | chialling | patchalling | boshaan | pantchiallings | salpetersoeker | overmaas | pantjalang | bonneratte | chialop | onbeschaamtheijt | pantc | patchall | patjallang | arnoldina | losboots | pantchall | desnoek | zijdeteeld | woelwater | suijkermaalder | bancq | depatchiall | kruisser | depant | debarcq | nacheribon | sorgdrager | zijdewoom | glisgis | beschutter | vantchiall | delosboot | garnaal | chailoup | beschermer | zordaan | galwet | casuaris | pandjallang | casuarus | pantj | schipio | galeij | oostendenaer | ontang | patch | burk | losboot | smapt | panthialling | bethij | breguantijn | depatch | coffijthuijn | pantsjall | contong | moesthuijn | ramsgatte | jallang | zuijerbeek | onbeschaamtheijd | pantchalling | panthiallang | pittoor | zuijkermaalder | chialoop | tanjongpour | vrctoria | vesuvius | pinxterbloem | chiloup | pantschiallang | </pre> In\u00a0[19]: Copied! <pre>for i, p in w2v.most_similar(\"amsterdam\", topn=100):\n    if p &gt;= 0.4:\n        print(i, end=\" \")\n</pre> for i, p in w2v.most_similar(\"amsterdam\", topn=100):     if p &gt;= 0.4:         print(i, end=\" \") <pre>sterdam middelburg amsterd amst zeeland amster amsterdm amstm rotterdam delft amsteldam enkhuijsen zeland middelburgh utrecht ams amsterda amste gravenhage terdam zeelant zeiland derwapen enchuijsen dam delff maddelburg middelb enckhuijsen amstedam enkhuijzen aamsterdam delfft presidiale enkhuisen seeland enckhuijzen geredreseert vlissingen rdam praesidiale amsterdan hage costeux zeelandt wappan hoorn rotterdant delburg delf delst behangsels inzeland middelbrerg enkhuizen proefidiaale praecidiale ceulen boodh caamer enckhuijs dewees behanghsel amsterstam temiddelburg enkhuysen zieland alkmaar meddelburg cognoissemet rotter sdh carode uijtgevaren middelburgin kameer delvt leijden zeel praesideale amstd uijtregt utregt hoplooper enchuy terkamer rabbinel vlissinge diale kaner veere arnhem confernee praesidiaale haarlem kamier enehuysen siemermeer middeburg amstdam </pre> In\u00a0[20]: Copied! <pre>for i in w2v.most_similar(\"intje\", topn=100):\n    print(i[0], end=\" | \")\n</pre> for i in w2v.most_similar(\"intje\", topn=100):     print(i[0], end=\" | \") <pre>jntje | maleijer | dul | maleyer | anachoda | bappa | salim | jntie | malijer | malaijer | malim | boeginees | jurragan | parnakan | iavaan | iuragan | intie | cadier | sadulla | carim | mochamat | abdul | samat | parnackan | javaan | arabier | assan | nachoda | javaen | soedin | bouginees | mohamat | abdulla | achmat | talip | iurragan | inw | kinko | balier | zait | jnw | lim | sleman | juragan | saijit | garrang | rahim | bagus | oeij | tjina | anach | njo | jabar | boeang | tjan | mahama | karim | boeijong | aboe | jnwoonder | ganie | campar | tja | garang | balijer | troena | kamat | mallijer | anak | chin | sait | cassim | machoda | boejong | soekoer | roekoe | nio | samara | oemar | poea | lebe | hoko | miskien | vrijbalier | maijang | hoeko | salee | sech | samsoe | boegenees | naghoda | koko | gonting | tenoedin | mandarees | oesien | troeno | draman | sinko | jamal | </pre> In\u00a0[21]: Copied! <pre>for i in w2v.most_similar(\"caneel\", topn=100):\n    print(i[0], end=\" | \")\n</pre> for i in w2v.most_similar(\"caneel\", topn=100):     print(i[0], end=\" | \") <pre>canneel | arreecq | arreeck | cardamom | geschilden | balen | cardaman | cardamon | areecq | arreek | geschilt | overjarigen | areek | geschilde | ruijnas | bast | kurkuma | wortelen | wortel | cardemom | cannel | saffragamse | arreeq | groven | jndigo | incorrecte | ammenams | schelders | plantjes | curcuma | areeck | ougsten | affpacken | zaije | runas | schillens | moernagelen | cauwa | wortels | smakeloose | koehuijden | klenen | indigo | gekookten | zalpeter | canneer | saije | calpentijnsen | cragtelose | endeneese | canneelschilders | cheijlonsen | kannee | reuck | baelen | baalen | kanneel | pingos | sacken | varssen | anijl | ruinas | ammonams | tabacq | zaat | cauris | amm | ruias | cardanom | fijnen | cardamam | coffijbonen | cardamoin | arreck | bhaalen | zaijen | nagelen | caneell | embaleeren | bladeren | berberijen | coffijboonen | overjarige | kleenen | fordeelen | zaad | onrijpe | noten | pken | specerije | gamsen | geschild | caaneel | roggevellen | endeneesche | ingesamelden | oliteiten | peerlen | pepen | elijhanten | </pre> In\u00a0[22]: Copied! <pre>for i in w2v.most_similar(\n    positive=[\"weder\", \"weer\", \"regen\"], negative=[\"wederom\", \"alweder\"], topn=100\n):\n    print(i[0], end=\" | \")\n</pre> for i in w2v.most_similar(     positive=[\"weder\", \"weer\", \"regen\"], negative=[\"wederom\", \"alweder\"], topn=100 ):     print(i[0], end=\" | \") <pre>weir | reegen | wint | zeewint | lugt | windt | noorde | winden | waaijende | stroom | buijen | winde | sneeuw | doorwaijen | zuijde | lucht | regenbuijen | waijende | wind | coeltjens | suijdweste | koude | vlagen | handsaem | weerligt | dewind | regenagtig | tegenstroom | doorwaijende | sonneschijn | regenen | stilte | koelte | regens | coelte | lught | hitte | stijve | lughje | zeewind | wintje | weste | warme | onstuijmig | reegenen | stroomen | koelten | zonnestraalen | delugt | warmte | handsaam | buijdige | travaden | doorbreken | inbreeken | moussom | doorwaaijende | reegende | travadig | doorstaande | doorkomende | hette | buijig | luchje | felle | afwatering | starke | kentering | overdag | stormwinden | reegens | wzw | westelijke | vloet | variable | coeltje | calte | tegenwinden | ooste | goedweer | oostelijke | noordweste | zot | waaijde | deijning | aartrijk | noordelijk | valwinden | ongestadige | doorwaaijen | slijve | suijde | caelte | lugties | firmament | regende | coeste | travodig | coelende | doorbrake | </pre> In\u00a0[23]: Copied! <pre>w2v.closer_than(\"eendracht\", \"tilburg\")\n</pre> w2v.closer_than(\"eendracht\", \"tilburg\") Out[23]: <pre>['ende',\n 'naer',\n 'oock',\n 'noch',\n 'int',\n 'nae',\n 'schepen',\n 'retour',\n 'vant',\n 'camer',\n 'gecomen',\n 'fluijt',\n 'volck',\n 'becomen',\n 'welck',\n 'jacht',\n 'hoorn',\n 'japan',\n 'rotterdam',\n 'coninck',\n 'ditto',\n 'jagt',\n 'wint',\n 'compe',\n 'godt',\n 'lant',\n 'eijlanden',\n 'derwaerts',\n 'end',\n 'vertreck',\n 'landt',\n 'goa',\n 'geladen',\n 'stadt',\n 'tschip',\n 'bat',\n 'comende',\n 'maent',\n 'opt',\n 'chaloup',\n 'maecken',\n 'ladinge',\n 'japara',\n 'delft',\n 'oocq',\n 'gearriveert',\n 'genaemt',\n 'gemaeckt',\n 'weijnich',\n 'coningh',\n 'rhede',\n 'langh',\n 'waermede',\n 'daermede',\n 'ene',\n 'macht',\n 'ancker',\n 'originele',\n 'jnt',\n 'eijlant',\n 'nassauw',\n 'augustij',\n 'vrede',\n 'quartieren',\n 'wapen',\n 'voijagie',\n 'cattij',\n 'middagh',\n 'achter',\n 'opde',\n 'vaderlant',\n 'portugees',\n 'geseijde',\n 'leeuw',\n 'dirck',\n 'cargasoen',\n 'verwachten',\n 'mauritius',\n 'rijck',\n 'chialoep',\n 'dach',\n 'namentlijck',\n 'eijlandt',\n 'geladene',\n 'vlissingen',\n 'jachten',\n 'battavia',\n 'gelyck',\n 'seecker',\n 'wingurla',\n 'gescheept',\n 'amst',\n 'portugesen',\n 'iapan',\n 'comste',\n 'stondt',\n 'nederlants',\n 'arent',\n 'nacht',\n 'vercocht',\n 'haven',\n 'zielen',\n 'caap',\n 'gedestineert',\n 'goens',\n 'fregat',\n 'vaert',\n 'oosten',\n 'galjoot',\n 'iagt',\n 'bouton',\n 'admirael',\n 'baij',\n 'aengecomen',\n 'eenich',\n 'orangie',\n 'besendinge',\n 'nde',\n 'batt',\n 'joncken',\n 'toecomende',\n 'journael',\n 'fortuijn',\n 'conincx',\n 'volladen',\n 'enkhuijsen',\n 'gevaren',\n 'amsterd',\n 'engeland',\n 'diemen',\n 'japanse',\n 'suratte',\n 'texel',\n 'souratte',\n 'vaeren',\n 'moluccos',\n 'hooch',\n 'cleen',\n 'vandaer',\n 'vaertuijgh',\n 'aent',\n 'noorden',\n 'vloote',\n 'windt',\n 'loo',\n 'noort',\n 'havenen',\n 'jager',\n 'jans',\n 'ganges',\n 'gedestineerde',\n 'jagtje',\n 'mallacca',\n 'westen',\n 'reyse',\n 'twelcke',\n 'helena',\n 'claes',\n 'swarten',\n 'sumatra',\n 'suijcker',\n 'speelman',\n 'datmen',\n 'concordia',\n 'lam',\n 'burgh',\n 'bon',\n 'geseijlt',\n 'geseijt',\n 'wech',\n 'ouer',\n 'wacht',\n 'princes',\n 'vijant',\n 'uijren',\n 'vertreckt',\n 'lanck',\n 'vaderlandt',\n 'sunda',\n 'suratta',\n 'koninck',\n 'dittos',\n 'naderhant',\n 'mette',\n 'spiegel',\n 'eendragt',\n 'welvaren',\n 'graden',\n 'metten',\n 'wester',\n 'cleijn',\n 'oorloge',\n 'hollandia',\n 'avondt',\n 'mars',\n 'chaloupen',\n 'queda',\n 'hoecker',\n 'engelsz',\n 'malcanderen',\n 'maen',\n 'ternate',\n 'lichten',\n 'eenighe',\n 'velsen',\n 'boeckhouder',\n 'hoeck',\n 'soodanich',\n 'eenelijck',\n 'aer',\n 'vijandt',\n 'fluijten',\n 'toegecomen',\n 'leijt',\n 'becoomen',\n 'pauw',\n 'pallas',\n 'stierman',\n 'seijlen',\n 'jaght',\n 'bengala',\n 'beer',\n 'gelicht',\n 'seijl',\n 'zuijt',\n 'nangasackij',\n 'lants',\n 'nederlantse',\n 'eijndelijck',\n 'anthonio',\n 'jonck',\n 'facture',\n 'anckers',\n 'monterende',\n 'esperance',\n 'schagen',\n 'jegenwoordich',\n 'opperhooffden',\n 'monteert',\n 'geprojecteert',\n 'eylanden',\n 'taijouan',\n 'persien',\n 'aengebracht',\n 'brant',\n 'macquian',\n 'straet',\n 'schep',\n 'ladingh',\n 'portugese',\n 'ouglij',\n 'alreede',\n 'lis',\n 'insgelijcx',\n 'maes',\n 'besettinge',\n 'macao',\n 'haas',\n 'geraeckt',\n 'fluijtje',\n 'caep',\n 'zeelandia',\n 'gemaect',\n 'joris',\n 'twelcq',\n 'selvige',\n 'spoedigh',\n 'doort',\n 'zeelant',\n 'nassau',\n 'retourneren',\n 'overgecomen',\n 'oorsaecke',\n 'bantham',\n 'middach',\n 'nodich',\n 'rechte',\n 'naert',\n 'martij',\n 'costi',\n 'welcq',\n 'overnes',\n 'voorss',\n 'waert',\n 'maendt',\n 'siams',\n 'verrichten',\n 'cauw',\n 'middelburgh',\n 'sillida',\n 'snoek',\n 'tidoor',\n 'brugge',\n 'zeijlende',\n 'voornt',\n 'ouwerkerk',\n 'naervolgende',\n 'conincq',\n 'aenstonts',\n 'wederomme',\n 'elck',\n 'gesecht',\n 'fredrick',\n 'cast',\n 'volcht',\n 'diamant',\n 'jaerlijcx',\n 'dieren',\n 'mouson',\n 'compste',\n 'opden',\n 'genoech',\n 'batavier',\n 'vaertuijch',\n 'geraecken',\n 'middelb',\n 'reste',\n 'chial',\n 'reviere',\n 'adrichem',\n 'castricum',\n 'tegenwoordich',\n 'vracht',\n 'margaretha',\n 'overschie',\n 'damme',\n 'spijk',\n 'grave',\n 'horst',\n 'dicht',\n 'biema',\n 'engelant',\n 'terstont',\n 'jagten',\n 'doornik',\n 'tocht',\n 'gecocht',\n 'siecken',\n 'lagh',\n 'amboijna',\n 'redelijck',\n 'nieuwland',\n 'verovert',\n 'arriveren',\n 'capelle',\n 'verwacht',\n 'gecregen',\n 'cargasoenen',\n 'houdt',\n 'scheepie',\n 'raap',\n 'ooc',\n 'hillegonda',\n 'overgescheept',\n 'onderwegen',\n 'hollantse',\n 'voorder',\n 'nassouw',\n 'vruchten',\n 'brandt',\n 'oma',\n 'westhoven',\n 'ongeluck',\n 'spiering',\n 'sijmon',\n 'goes',\n 'fluijtschip',\n 'selue',\n 'reij',\n 'geankert',\n 'bort',\n 'hercules',\n 'verbij',\n 'hoet',\n 'gelijcq',\n 'gezeijlt',\n 'gesamentlijck',\n 'vrijburg',\n 'manilha',\n 'havens',\n 'thoff',\n 'oudt',\n 'naderhandt',\n 'negombo',\n 'walcheren',\n 'opperdoes',\n 'breda',\n 'pegu',\n 'solor',\n 'joncke',\n 'mallebaer',\n 'chiampan',\n 'voorspoet',\n 'redout',\n 'naerde',\n 'batavise',\n 'boa',\n 'jappan',\n 'briel',\n 'abbekerk',\n 'sont',\n 'tijdingh',\n 'weynich',\n 'geseth',\n 'meijnden',\n 'vliegende',\n 'woerden',\n 'vaderlantse',\n 'genaempt',\n 'wilhem',\n 'achteren',\n 'paliacatta',\n 'steecken',\n 'alst',\n 'boero',\n 'zeelandt',\n 'langewijk',\n 'riviere',\n 'formosa',\n 'adelborst',\n 'mettet',\n 'mett',\n 'caron',\n 'leck',\n 'tjagt',\n 'ano',\n 'snauw',\n 'broeck',\n 'tjacht',\n 'erasmus',\n 'manilhas',\n 'witten',\n 'beverwijk',\n 'utrecht',\n 'quaemen',\n 'gouda',\n 'cats',\n 'atchin',\n 'ingeladen',\n 'enckhuijsen',\n 'verricht',\n 'coen',\n 'vlaming',\n 'claesz',\n 'larique',\n 'brandenburg',\n 'grondt',\n 'molucco',\n 'aendoen',\n 'come',\n 'duijnen',\n 'vuijt',\n 'armade',\n 'iacoba',\n 'boucken',\n 'firando',\n 'moij',\n 'unie',\n 'dolphijn',\n 'daerse',\n 'foreest',\n 'kat',\n 'thof',\n 'joncq',\n 'geruchten',\n 'haen',\n 'cha',\n 'flora',\n 'amb',\n 'spoedich',\n 'keulen',\n 'saterdagh',\n 'janssen',\n 'campen',\n 'aencomste',\n 'vaerwater',\n 'date',\n 'standt',\n 'haerlem',\n 'fluyt',\n 'chaloep',\n 'jaccatra',\n 'dienstich',\n 'bassora',\n 'veroverde',\n 'deense',\n 'aenboort',\n 'castel',\n 'baije',\n 'assenburg',\n 'reeckeninge',\n 'buuren',\n 'hoochte',\n 'paert',\n 'besendingh',\n 'bellasoor',\n 'hurdt',\n 'africa',\n 'engelandt',\n 'barentsz',\n 'drije',\n 'gemant',\n 'victorie',\n 'coeverden',\n 'gescheepte',\n 'etmael',\n 'borsselen',\n 'baeij',\n 'samson',\n 'inlants',\n 'zunda',\n 'besuijden',\n 'nieuwerkerk',\n 'valck',\n 'blijdorp',\n 'chialoepen',\n 'eten',\n 'redelijcke',\n 'mitsgaeders',\n 'delfs',\n 'oosthuijsen',\n 'sulckx',\n 'henrick',\n 'westerbeek',\n 'brugh',\n 'nas',\n 'tland',\n 'vaem',\n 'riff',\n 'antonio',\n 'hollant',\n 'nederlantsche',\n 'schulp',\n 'cruijs',\n 'verongelucken',\n 'wassenaar',\n 'londen',\n 'nachts',\n 'beschermer',\n 'geraackt',\n 'outshoorn',\n 'maendagh',\n 'wercq',\n 'nederlant',\n 'opperstierman',\n 'rijckloff',\n 'gecombineert',\n 'aengeweest',\n 'naght',\n 'voorburg',\n 'strandt',\n 'waveren',\n 'reduijt',\n 'cregen',\n 'noordbeek',\n 'draak',\n 'jaques',\n 'suijder',\n 'ellemeet',\n 'nova',\n 'nuijts',\n 'gelaeden',\n 'pool',\n 'besoecken',\n 'vosmaar',\n 'castor',\n 'retourneert',\n 'oct',\n 'schepe',\n 'comptanten',\n 'herstelde',\n 'onderzeijl',\n 'wickenburg',\n 'popkensburg',\n 'oorloch',\n 'steenhoven',\n 'vlamingh',\n 'bonne',\n 'seijnden',\n 'werwaerts',\n 'straalen',\n 'bijt',\n 'theodora',\n 'kercke',\n 'hogersmilde',\n 'tanjongpoura',\n 'datelijck',\n 'terra',\n 'iager',\n 'derwarts',\n 'dto',\n 'rotterd',\n 'aengeland',\n 'cijlon',\n 'soot',\n 'gideon',\n 'gevolcht',\n 'vertrocke',\n 'samarangh',\n 'hoedanich',\n 'gestevent',\n 'hollandt',\n 'herwarts',\n 'meerman',\n 'wesel',\n 'alphen',\n 'raeckende',\n 'iacht',\n 'twapen',\n 'alsvooren',\n 'parsia',\n 'ledigh',\n 'amstel',\n 'enchuijsen',\n 'masulipatam',\n 'rosingijn',\n 'vane',\n 'vestinge',\n 'papenburg',\n 'ontladen',\n 'valkenisse',\n 'reeckeningh',\n 'voorgemelte',\n 'tarnaten',\n 'gangh',\n 'laeden',\n 'hoochsten',\n 'ock',\n 'breecken',\n 'robbertus',\n 'booth',\n 'peerl',\n 'gouw',\n 'eylant',\n 'schelde',\n 'langhs',\n 'boordt',\n 'wendela',\n 'eenhoorn',\n 'voorland',\n 'faam',\n 'visvliet',\n 'beoosten',\n 'iagtje',\n 'salm',\n 'clachten',\n 'gevaeren',\n 'perijckel',\n 'redel',\n 'naet',\n 'seijlon',\n 'alsem',\n 'bocht',\n 'besich',\n 'leyden',\n 'groeningen',\n 'haerder',\n 'carthago',\n 'vruchteloos',\n 'maleijo',\n 'block',\n 'hardt',\n 'rhoon',\n 'wijck',\n 'genoechsaem',\n 'oostrust',\n 'waerdich',\n 'pollux',\n 'verseeckeringe',\n 'gesicht',\n 'mane',\n 'aancomste',\n 'geduijrende',\n 'besettingh',\n 'portugael',\n 'passerende',\n 'raadhuijs',\n 'rijcke',\n 'cargo',\n 'vertrecq',\n 'lampon',\n 'scholtenburg',\n 'palimbangh',\n 'deensche',\n 'amerongen',\n 'veroorsaeckt',\n 'enge',\n 'bogaart',\n 'choromandel',\n 'chirrebon',\n 'delff',\n 'bijweg',\n 'landskroon',\n 'alsnoch',\n 'bevindingh',\n 'menichte',\n 'sint',\n 'beseth',\n 'vijants',\n 'ridderkerk',\n 'westerveld',\n 'schiedam',\n 'beekvliet',\n 'comptant',\n 'padmos',\n 'retourneerende',\n 'naart',\n 'geseijden',\n 'aencomende',\n 'nederlandt',\n 'aden',\n 'becommen',\n 'maccauw',\n 'byden',\n 'crab',\n 'cronenburg',\n 'aenkomste',\n 'vlucht',\n 'eylandt',\n 'haes',\n 'waerden',\n 'purmer',\n 'correcorren',\n 'tack',\n 'majt',\n 'alsnu',\n 'verseijlt',\n 'zuratta',\n 'slach',\n 'scheepken',\n 'strijen',\n 'middelwout',\n 'verwachtende',\n 'zeepaard',\n 'brachten',\n 'beijeren',\n 'dregterland',\n 'geertruij',\n 'casar',\n 'javan',\n 'ceres',\n 'aencomen',\n 'hendricksz',\n 'tpatria',\n 'snachts',\n 'gevolght',\n 'ida',\n 'eijl',\n 'jonghst',\n 'triton',\n 'mandorijn',\n 'voorwaer',\n 'ome',\n 'corts',\n 'spaens',\n 'meerlust',\n 'dircksz',\n 'merwe',\n 'voorschooten',\n 'patanij',\n 'hogenes',\n 'borneo',\n 'eem',\n 'vertimmeren',\n 'vlote',\n 'nederhoven',\n 'rosenburg',\n 'jnlants',\n 'oranje',\n 'fregatten',\n 'ria',\n 'naerden',\n 'maldives',\n 'beveland',\n 'bodt',\n 'selffde',\n 'daechs',\n 'nass',\n 'negrij',\n 'purmerlust',\n 'uijr',\n 'overcomste',\n 'vaderlants',\n 'goas',\n 'ceulen',\n 'veroveren',\n 'jerusalem',\n 'defluijt',\n 'limburg',\n 'geseyde',\n 'hopvogel',\n 'rebecca',\n 'azia',\n 'chiampans',\n 'punct',\n 'veere',\n 'gaasperdam',\n 'uno',\n 'crap',\n 'lastdrager',\n 'sleewijk',\n 'verbrant',\n 'suijt',\n 'suijckeren',\n 'phenix',\n 'vicq',\n 'padtbrugge',\n 'lach',\n 'januario',\n 'aengelant',\n 'zuijderburg',\n 'medebrengende',\n 'geanckert',\n 'malcander',\n 'merckt',\n 'coomende',\n 'voorstaende',\n 'aes',\n 'maleijen',\n 'osacca',\n 'vreeland',\n 'leste',\n 'suijd',\n 'batavi',\n 'daghregister',\n 'partie',\n 'terdam',\n 'schoonderloo',\n 'cruijssen',\n 'maleije',\n 'calpentijn',\n 'vriesland',\n 'geus',\n 'renswoude',\n 'kerkwijk',\n 'westerdijxhorn',\n 'delfshaven',\n 'oostende',\n 'geroerde',\n 'vlagh',\n 'zeel',\n 'gissingh',\n 'geberchte',\n 'duijnenburg',\n 'sar',\n 'fluijtie',\n 'herstelder',\n 'duijven',\n 'schuijtwijk',\n 'thuys',\n 'aatchin',\n 'tcasteel',\n 'verongeluckte',\n 'vojagie',\n 'vrientschap',\n 'horstendaal',\n 'loenderveen',\n 'dordrecht',\n 'verseijlen',\n 'liggen',\n 'milde',\n 'madrast',\n 'aleppo',\n 'pantchiall',\n 'nieuwstad',\n 'barbara',\n 'meijenberg',\n 'dieshoek',\n 'carpentier',\n 'gevlucht',\n 'brack',\n 'weeck',\n 'schoonauwen',\n 'bombahia',\n 'macas',\n 'geertruijd',\n 'voorschoten',\n 'amadabath',\n 'mathijs',\n 'beeck',\n 'schellag',\n 'spoedichste',\n 'sparenrijk',\n 'graeff',\n 'jacatra',\n 'uijtgaen',\n 'voyagie',\n 'diemermeer',\n 'stieren',\n 'mourits',\n 'jorisz',\n 'gemaeckte',\n 'gestadich',\n 'aetchin',\n 'jonghste',\n 'schips',\n 'junius',\n 'crooswijk',\n 'andragirij',\n 'paauw',\n 'bellesoor',\n 'berkenroode',\n 'mijdregt',\n 'robijn',\n 'naede',\n 'deventer',\n 'sielen',\n 'kroonenburg',\n 'mondt',\n 'gheijn',\n 'ziam',\n 'hulst',\n 'palembangh',\n 'ael',\n 'verwachte',\n 'vegt',\n 'midts',\n 'alrede',\n 'superintendent',\n 'horssen',\n 'pampus',\n 'laer',\n 'zuyd',\n 'aengebrachte',\n 'caeb',\n 'hittoe',\n 'raecken',\n 'daman',\n 'prattenburg',\n 'frederick',\n 'vlotter',\n 'lichte',\n 'gecombineerd',\n 'landts',\n 'doradus',\n 'jamby',\n 'vroech',\n 'putmans',\n 'bril',\n 'maetsuijcker',\n 'maccao',\n 'delfland',\n 'kronenburg',\n 'bengaele',\n 'makassar',\n 'salanghoor',\n 'proostwijk',\n 'rescontre',\n 'perde',\n 'rosairo',\n 'tydinge',\n 'gemerct',\n 'ijsselmonde',\n 'giroffel',\n 'kiefhoek',\n 'eend',\n 'portugiesen',\n 'joncquen',\n 'tijdelijk',\n 'velzen',\n 'duijvenvoorde',\n 'belois',\n 'vervolch',\n 'gerescontreert',\n 'lieffde',\n 'tulpenburg',\n 'wart',\n 'gracht',\n 'admiraals',\n 'poedecherij',\n 'rotter',\n 'scheijbeek',\n 'aengeb',\n 'lacca',\n 'quelangh',\n 'veldhoen',\n 'gesocht',\n 'arriv',\n 'alblasserdam',\n 'meijnde',\n 'stolle',\n 'maeckten',\n 'ginck',\n 'geluckigh',\n 'dinsdagh',\n 'vlieger',\n 'tijger',\n 'xula',\n 'haij',\n 'naegelen',\n 'haring',\n 'hoochste',\n 'brenght',\n 'veerman',\n 'swaen',\n 'swol',\n 'diana',\n 'bentvelt',\n 'laus',\n 'bogaert',\n ...]</pre> In\u00a0[24]: Copied! <pre>for i in w2v.most_similar(\"regen\", topn=100):\n    print(i[0], end=\" | \")\n</pre> for i in w2v.most_similar(\"regen\", topn=100):     print(i[0], end=\" | \") <pre>reegen | regens | reegens | droogte | continueelen | regentijd | hitte | hette | afwatering | continuelen | felle | travaden | regentijt | heete | koude | opperwater | sonneschijn | weste | swaren | buijen | vloed | overvloedigen | regenbuijen | coortse | gestadigen | onstuijmig | doorbreken | regenen | afwateringe | weir | stilte | waijende | westelijke | pides | onweer | vlagen | warmte | tegenwinden | noorde | regenagtig | koortsen | aardbeving | ooste | winden | stormwinden | sneeuw | stroomen | oostelijke | veroorsaakte | dampen | opkomende | stormen | doorwaijende | valwinden | oostewinden | vloet | waaijende | koors | stiltens | winde | vaarbaar | lucht | doorwaijen | hete | stromen | getij | doorblasende | regenagtigh | tegenwind | fellen | geduurigen | delugt | swaaren | moussom | reegentijd | vuurberg | suijdweste | stilten | inbreeken | ongestadig | aanwakkerende | overkroptheijt | aartrijk | westelijcke | ongemeenen | defelle | kentering | broeijende | weerligt | continuele | afwateringen | swaeren | doorstaande | schrale | zuijde | begonde | verlopen | reegenen | noordweste | handsaam | </pre> In\u00a0[25]: Copied! <pre>for i in w2v.most_similar(\"schipbreuk\", topn=100):\n    print(i[0], end=\" | \")\n</pre> for i in w2v.most_similar(\"schipbreuk\", topn=100):     print(i[0], end=\" | \") <pre>machteloos | jammerlijk | honger | uijtgestaen | breuk | ongemak | schade | tempeest | onweer | accident | aardbevingh | ongeluk | orcaan | onveer | amsters | calamiteijt | koorts | woedend | crimiineelen | brandinge | gesucceld | schaade | ongemack | onweder | schaede | vreese | deerlyk | storm | uitgestaan | swaaren | ongeluck | bhuij | rampe | elende | besprongen | tanjepoer | ellendig | orcanen | vesemente | zeerampen | vrees | nootweer | stormen | geblasen | ongebal | geplaegt | gewopen | aardbeving | vesementen | presumeerden | uijtwendig | aardbevinge | jarigie | storme | hartseer | monding | deerlijck | vreeze | travade | gewaeijt | stooting | affront | eytmatrauw | stoting | ellende | mack | arrepo | deerlijk | bloedigen | naod | vrese | travaat | vruchte | uijtgestaane | louter | swaeren | smaadheden | travaet | pachtert | travaden | holgaende | smaet | dewijlen | flaauwten | aensegen | boegh | onweeder | belaglijk | nagejaagt | gaets | hongersnoot | hottentoosen | inflamatie | onderlek | losson | nederlage | rotterdame | tcelamse | verbolgen | jaagt | </pre> In\u00a0[26]: Copied! <pre>for i in w2v.most_similar(\"pieter\", topn=100):\n    print(i[0], end=\" | \")\n</pre> for i in w2v.most_similar(\"pieter\", topn=100):     print(i[0], end=\" | \") <pre>gerrit | cornelis | paulus | ian | leendert | barent | jan | evert | andries | lourens | claas | marten | daniel | roeloff | anthonij | dirk | harmanus | theunis | lambert | joost | sijmon | roelof | albert | martinus | gillis | michiel | matthijs | jacob | maerten | govert | maarten | harmen | abraham | iacobus | johannes | volkert | carsten | barend | dirck | rijnier | huijbert | jacobus | hendrick | jasper | abram | egbert | jurriaan | christiaen | sijbrand | verhoef | siewert | arnoldus | laurens | samuel | anthoni | iacob | nicolaas | meijndert | marinus | lucas | coert | iurriaan | hermanus | gerbrant | bartholomeus | henderik | iohannes | isaak | jochem | christiaan | eldert | harman | amos | reijer | guilliam | ioost | gilles | david | antonij | hendrik | reijndert | corthals | hend | bartel | aarnout | arent | casper | joris | jurriaen | coenraet | johannis | adam | adriaen | noach | adriaan | poulus | warnar | anthony | wessel | iurriaen | </pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#word2vec-experiment","title":"Word2Vec Experiment\u00b6","text":""},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#data","title":"Data\u00b6","text":"<p>The data can be downloaded from the GLOBALISE Dataverse: https://datasets.iisg.amsterdam/dataverse/globalise. For this experiment, we're working with version V2.0 of the Transcription dataset:</p> <ul> <li>GLOBALISE project, 2024, \"VOC transcriptions v2 - GLOBALISE\", https://hdl.handle.net/10622/LVXSBW, IISH Data Collection</li> </ul> <p>The project conveniently provides a file with pointers to all txt files in this dataset that we can download automatically. We are using <code>wget</code> to download the files. First the file with pointers, which we will use to download all txt files. This can take a while.</p>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#pre-processing","title":"Pre-processing\u00b6","text":""},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#processing","title":"Processing\u00b6","text":""},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#loading-a-pretrained-model","title":"Loading a pretrained model\u00b6","text":"<p>There's a slight difference in what we generate above, and what we can load below. To streamline the commands, we load it as a KeyedVectors object. Also, this is the place to load an earlier trained model.</p>"},{"location":"experiments/GLOBALISE_Word2Vec_Lab/#analysis","title":"Analysis\u00b6","text":""},{"location":"experiments/blacklab-search-interface-general-missives/","title":"BlackLab Search Interface for the General Missives of the VOC","text":"<p>Date: 2021 (pre-GLOBALISE) URL: https://corpora.ato.ivdnt.org/corpus-frontend/Missiven/search Status: Production People involved: Sophie Arnoult, Jesse de Does, Dirk Roorda, Jan Niestadt, Lodewijk Petram, Piek Vossen, Jessica den Oudsten, Dani\u00ebl Tuik</p> <p>A BlackLab search environment offers a new way to explore the General Missives of the Dutch East India Company (VOC). These reports, sent from Batavia (Jakarta) to the Dutch Republic between 1610 and 1795, are now accessible for in-depth research thanks to efforts within the CLARIAH project by a team from VU University, the Huygens Institute, and the Dutch Language Institute. Utilizing advanced OCR and Named Entity Recognition techniques <sup>1</sup>, the team enhanced these documents with metadata and structural elements, including annotations for entities like persons and locations.</p> <p>The BlackLab interface facilitates computational analysis and robust search capabilities of the enriched texts. Researchers can now perform complex syntactic searches or simple keyword queries, uncovering nuanced historical and linguistic insights. A slightly cleaner version of the same corpus is also available as a Text-Fabric resource.</p> <p> https://corpora.ato.ivdnt.org/corpus-frontend/Missiven/search</p> <p>The General Missives summarize the information contained in the Overgekomen Brieven en Papieren series of documents from the VOC archives that the GLOBALISE project aimes to unlock for in-depth research. The corpus available in the BlackLab environment is a selection of General Missives from the period 1610-1767 that was transcribed, edited and published in 14 (digital) book volumes by the Huygens Institute and its predecessors. The original volumes are also available online. </p> <p>Please note that the General Missives contain labels, characterizations and information about persons, actions and events that may be offensive and troubling to individuals and communities.</p> <ol> <li> <p>Sophie I. Arnoult, Lodewijk Petram, and Piek Vossen. 2021. Batavia asked for advice. Pretrained language models for Named Entity Recognition in historical texts. In Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 21\u201330, Punta Cana, Dominican Republic (online). Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.latechclfl-1.3 \u21a9</p> </li> </ol>"},{"location":"experiments/htr-viewer/","title":"GLOBALISE Transcriptions Viewer","text":"<p>Date: October 4, 2023 URL: https://transcriptions.globalise.huygens.knaw.nl/ Status: Prototype People involved: Sebastiaan van Daalen, Hayco de Jong, Bram Buitendijk, Hennie Brugman, Arno Bosse, Leon van Wissen, Lodewijk Petram</p> <p>The aim of the GLOBALISE project is to facilitate research with the Overgekomen Brieven en Papieren series of documents from the VOC archives. As a first step to reaching this goal, we generate transcriptions of the c. 5 million of handwritten pages made available by the Dutch National Archives using automatic transcription software. </p> <p>While we publish text files of the transcriptions on the GLOBALISE Dataverse, we also experiment with building an interface for easy searching and exploring the material. A first prototype can be accessed through the link below. Please share your feedback through our contact form. In the future, improved versions will be made available.</p> <p> https://transcriptions.globalise.huygens.knaw.nl/</p> <p>The collection of archival documents made available in the viewer comprises inventory numbers 1053-4454 and 7527-11024 from the VOC archives, National Archives, The Hague. The scans of the original documents (n=4,802,212) from the period 1610-1796 are available on the website of the National Archives.</p> <p>Please note that the transcriptions will contain errors. They have not been manually checked for accuracy or completeness. Some labels, characterizations and information about persons, actions and events may be offensive and troubling to individuals and communities. Be careful when relying on these transcriptions and be aware of their limitations.</p>"},{"location":"experiments/places-visualization/","title":"GLOBALISE Places Visualization","text":"<p>Date: May 2023 URL: https://globalise.shinyapps.io/mapping_places/ Status: Prototype People involved: Ruben Land</p> <p>Initially as an intern at the GLOBALISE project and now as a student assistant, Ruben Land is working on a dataset of places that occur in the Overgekomen Brieven en Papieren series of VOC documents. He uses R Shiny to create interactive visualizations of his work. These can be accessed by clicking the image below.</p> <p> https://globalise.shinyapps.io/mapping_places/</p>"},{"location":"experiments/skosmos-concept-browser/","title":"Thesaurus concepts browser","text":"<p>We\u2019re working on developing a GLOBALISE thesaurus with definitions of concepts that occur in the Overgekomen Brieven en Papieren series of VOC documents. A preliminary version of the thesaurus can be explored in our SKOSMOS environment.<sup>1</sup></p> <p>Please note that the thesaurus is constantly being improved and extended, and the the URIs in the current version are not stable.</p> <p></p> <ol> <li> <p>This demo is running the SKOSMOS software, developed by the National Library of Finland, to provide a user-friendly interface to our thesaurus. The SKOSMOS software is open source and available on GitHub.\u00a0\u21a9</p> </li> </ol>"},{"location":"experiments/text-fabric-general-missives/","title":"Text-Fabric Serialization of the General Missives of the VOC","text":"<p>Date: 2022 (pre-GLOBALISE) URL: https://clariah.github.io/wp6-missieven-search/text/index.html and https://github.com/CLARIAH/wp6-missieven/ Status: Demo People involved: Dirk Roorda, Sophie Arnoult, Lodewijk Petram, Piek Vossen, Jesse de Does, Jessica den Oudsten, Dani\u00ebl Tuik</p> <p>A Text-Fabric representation of the General Missives of the Dutch East India Company (VOC) offers a new way to explore and analyze these reports. The General Missives sent from Batavia (Jakarta) to the Dutch Republic between 1610 and 1795, are now accessible for in-depth research thanks to efforts within the CLARIAH project by a team from VU University, the Huygens Institute, and the Dutch Language Institute. Utilizing advanced OCR and Named Entity Recognition techniques <sup>1</sup>, the team enhanced these documents with metadata and structural elements, including annotations for entities like persons and locations.</p> <p>The Text-Fabric serialization of the enriched texts is especially suited for linguistic analysis with computational methods. Users can explore the materials in the Text-Fabric search interface or by using the Text-Fabric Python package. A slightly less cleaned version of the same corpus is also available in a BlackLab search environment.</p> <p> https://clariah.github.io/wp6-missieven-search/text/index.html</p> <p>The General Missives summarize the information contained in the Overgekomen Brieven en Papieren series of documents from the VOC archives that the GLOBALISE project aimes to unlock for in-depth research. The corpus available in the BlackLab environment is a selection of General Missives from the period 1610-1767 that was transcribed, edited and published in 14 (digital) book volumes by the Huygens Institute and its predecessors. The original volumes are also available online. </p> <p>Please note that the General Missives contain labels, characterizations and information about persons, actions and events that may be offensive and troubling to individuals and communities.</p> <ol> <li> <p>Sophie I. Arnoult, Lodewijk Petram, and Piek Vossen. 2021. Batavia asked for advice. Pretrained language models for Named Entity Recognition in historical texts. In Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 21\u201330, Punta Cana, Dominican Republic (online). Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.latechclfl-1.3 \u21a9</p> </li> </ol>"}]}